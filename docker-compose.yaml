version: "3.3"

x-common: &airflow-common
  build:
    context: ./docker/airflow
    dockerfile: Dockerfile
  user: "${AIRFLOW_UID:-50000}:0"
  env_file:
    - .env
  volumes:
    - ./code:/opt/airflow/code
    - ./pipeline/dags:/opt/airflow/dags
    - ./data/airflow/logs:/opt/airflow/logs
    - ./data/airflow/config:/opt/airflow/config
    - ./data/airflow/plugins:/opt/airflow/plugins
    - ./data/airflow/data:/data
    - ./data/uscities.csv:/opt/airflow/data/uscities.csv
  networks:
    - services

services:
  postgres:
    image: postgres:16-alpine
    container_name: postgres-airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - services
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
    volumes:
      - airflow-postgres-volume:/var/lib/postgresql/data

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-apiserver:
    <<: *airflow-common
    container_name: airflow-apiserver
    command: api-server
    restart: always
    ports:
      - "13005:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 30s
      retries: 5
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    depends_on:
      postgres:
        condition: service_healthy
    command:
      - -c
      - |
        # Đặt AIRFLOW_UID (User ID) nếu chưa được cung cấp từ file .env
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo "WARNING: AIRFLOW_UID not set, defaulting to $$(id -u)"
          export AIRFLOW_UID=$$(id -u)
        fi

        # 1. Tạo các thư mục dùng chung (bao gồm cả /code và /data của bạn)
        mkdir -p /opt/airflow/{logs,dags,plugins,config,code} /data

        # 2. Tạo file airflow.cfg mặc định (nếu chưa có)
        /entrypoint airflow config list >/dev/null

        # 3. Thay đổi chủ sở hữu của các thư mục
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config,code} /data

    environment:
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    user: "0:0"

  postgres-main:
    image: postgres:16-alpine
    container_name: "postgres-main"
    hostname: "postgres-main"
    ports:
      - "65432:5432"
    environment:
      POSTGRES_USER: ${APP_DB_USER}
      POSTGRES_PASSWORD: ${APP_DB_PASSWORD}
      POSTGRES_DB: ${APP_DB_NAME}
    volumes:
      - postgres-main-volume:/var/lib/postgresql/data
      - ./database/postgres_tables.sql:/docker-entrypoint-initdb.d/postgres_tables.sql
    command: ["postgres", "-c", "wal_level=logical"]
    healthcheck:
      test: ["CMD", "psql", "-U", "postgres", "-c", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - services

  kafka-broker:
    image: confluentinc/cp-kafka:7.8.3
    hostname: kafka-broker
    container_name: kafka-broker
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      #Kraft mode
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka-broker:9093"
      KAFKA_LISTENERS: "INTERNAL://:19092,EXTERNAL://:9092,DOCKER://:29092,CONTROLLER://:9093"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_CLUSTER_ID: "1L6g7nGhU-eAKfL--X25wo"
      CLUSTER_ID: "1L6g7nGhU-eAKfL--X25wo"

      #Cấu hình Listeners
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-broker:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    healthcheck:
      test: nc -z localhost 9092 || exit -1
      start_period: 15s
      interval: 5s
      timeout: 10s
      retries: 10
    networks:
      - services
    volumes:
      - kafka-volume:/var/lib/kafka/data

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    depends_on:
      kafka-broker:
        condition: service_healthy
    networks:
      - services
    ports:
      - "8095:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker:19092

  debezium:
    image: debezium/connect:2.7.3.Final
    container_name: debezium
    hostname: debezium
    depends_on:
      postgres-main:
        condition: service_healthy
      kafka-broker:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka-broker:19092
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      STATUS_STORAGE_TOPIC: connect_statuses
      OFFSET_STORAGE_TOPIC: connect_offsets
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      ENABLE_DEBEZIUM_SCRIPTING: "true"
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "--silent",
          "--fail",
          "-X",
          "GET",
          "http://debezium:8083/connectors",
        ]
      start_period: 10s
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - services

  debezium-ui:
    image: debezium/debezium-ui:latest
    container_name: debezium-ui
    hostname: debezium-ui
    depends_on:
      debezium:
        condition: service_healthy
    ports:
      - "8085:8080"
    environment:
      KAFKA_CONNECT_URIS: http://debezium:8083
    networks:
      - services

  debezium-connector-init:
    image: curlimages/curl:latest
    container_name: debezium-connector-init
    depends_on:
      debezium:
        condition: service_healthy
    volumes:
      - ./docker/debezium/init-scripts:/init-scripts
      - ./docker/debezium/init-scripts/postgres-connector.json:/init-scripts/postgres-connector.json
    command: sh -c '/init-scripts/init.sh'
    networks:
      - services

volumes:
  airflow-postgres-volume:
  postgres-main-volume:
  kafka-volume: # Thêm volume cho Kafka

networks:
  services:
    name: services